version: '3.9'
services:
  redis:
    # Container name is used to access locally
    container_name: redis_container
    image: redis:alpine
    # If there's an issue, we're restarting the container i.e if it fails, we
    # are going to restart
    restart: always
    ports:
      # Redis port is: 6379 inside the container and we're mapping it to 6379
      # outside the container i.e second is the port we're going to use to
      # access the service outside the container
      - '6379:6379'
    command: redis-server --loglevel warning
    # Since we're running this locally, we need a way for a docker to store
    # the data because Redis is like a database. So the data needs to be stored
    # somewhere. And we can do that by creating volumes i.e Volumes are like
    # storage where Docker stores data. Since Redis is a DB and we are running
    # locally, data can be stored using volumes. Here we're creating local
    # volume, so the volume will be stored in the same directory. So volume
    # will contain all the information that we're sending to Redis.
    volumes:
      # volumes takes path so inside this dir, there's going to be a folder
      # called Docker volumes and then inside, there's going to be cached data
      # and we're going to map it to /data
      # This docker volume will be created when we run this service. We don't
      # need to create it. And then also its going to create this cache dir.
      # Now what this means is:
      # This is local -> `./docker-volumes/cache`
      # And this is what we're getting from Redis -> `:data`
      # So the path /data from Redis, we want to map it to Docker volumes /cache
      # on our local machine. So where the data is supposed to be stored on
      # Redis, we are mapping it to this local path on our local machine.
      - ./docker-volumes/cache:/data
  # Service name can be whatever we want
  mongodb:
    container_name: mongodb_container
    # Version latest is fine for local development
    image: mongo:latest
    restart: always
    # INITDB ROOT USERNAME and PASSWORD is optional for local development
    # We can run MongoDB without that because not required while developing
    # locally. But if we want to take it to the cloud, like maybe we want to
    # use managed service i.e we want to manage the service ourself on
    # Kubernetes, then we definitely have to configure our DB. Local development
    # works fine without environment variables.
    ports:
      # MongoDB port number is 27017. So port number within the container and
      # port number we're mapping outside the container.
      - 27017:27017
    # We need to map the volume where we want the data to be stored on our
    # local machine.
    # From documentation for mongo docker image, MongoDB volumes are usually
    # mapped to `/data/db`
    # So we're going to map out own local volume to the mongodb data path
    volumes:
      # We're using the same path i.e `./docker-volumes` so we just want one
      # folder
      # This is what will be created on our local machine -> `./docker-volumes/data`
      # And this is what we're mapping that data to -> `/data/db`
      # That's 1 to 1 mapping. i.e whatever is contained in `/data/db`, we want
      # to add it to this path `./docker-volumes/data`
      # `/data` could be anything but I chose to use standard naming convention
      - ./docker-volumes/data:/data/db
  mysql:
    container_name: mysql_container
    # If tag is not used, it'll use :latest image tag by default
    image: mysql
    # Local development
    # For MySQL version 8.0 or higher, this command isn't required. It might
    # instead conflict with mysql server start.
    # command: --default-authentication-plugin=mysql_native_password
    restart: always
    # If we want to access MySQL from our application, then we use all of this
    # credentials
    environment:
      - MYSQL_USER=colson
      - MYSQL_DATABASE=tradenexus_auth
      # Use strong password for production
      - MYSQL_ROOT_PASSWORD=stillhome
      - MYSQL_PASSWORD=stillhome
    ports:
      - '3306:3306'
    volumes:
      - ./docker-volumes/mysql:/var/lib/mysql
  postgres:
    container_name: postgres_container
    image: postgres
    # For postgres, we don't need any command
    restart: always
    environment:
      - POSTGRES_USER=colson
      - POSTGRES_PASSWORD=stillhome
      - POSTGRES_DB=tradenexus_reviews
    ports:
      - '5432:5432'
    volumes:
      - ./docker-volumes/postgres:/var/lib/postgresql
  # For RabbitMQ, we're going to use the management-alpine version so we can
  # have access to management dashboard i.e we can see the UI for RabbitMQ.
  # For RabbitMQ itself, I'm not going to be storing any data. So I'm not
  # creating any volumes. It's possible with RabbitMQ if we really wanna make
  # our queue very durable. So that, in case there is a crash or there's a
  # restart, we can always get our data.
  # Here I won't be storing data. It will pass the data or the events that
  # I'm sending, will just pass through the queue.
  # Also, we need a default user and default password so that we can use it to
  # connect to RabbitMQ from our application and also use it to access the
  # management dashboard.
  rabbitmq:
    container_name: rabbitmq_container
    # image: rabbitmq:3.13-rc-management-alpine
    image: rabbitmq:4.1-rc-management-alpine
    restart: always
    environment:
      - RABBITMQ_DEFAULT_USER=colson
      - RABBITMQ_DEFAULT_PASS=stillhome
    # I'm going to set two different ports: AMQP protocol port and management
    # port. The protocol port will be the port we use to access from our
    # application and then the management port will be the port we use to
    # access our dashboard.
    ports:
      # AMQP protocol port
      - '5672:5672'
      # Management UI port
      - '15672:15672'
  # For Elasticsearch version ^8.11.0 and above, we need to enable security
  # on Elasticsearch and create a Kibana token for Kibana to properly connect
  # to Elasticsearch.
  # Otherwise it might not work proprety and we might experience the issue
  # where Kibana does not connect and on the browser we see the message
  # 'Kibana server is not ready yet' after waiting a long time.
  # For Elasticsearch, we're only creating a single node cluster for development
  # If using Docker Desktop -> Increase the CPU Limit to maximum and Memory
  # Limit to atleast 4 GB or better 12 GB
  # Also make sure the Elasticsearch version is at least 8. Don't use v7.x.x
  # Lot of things has changed. On v7, we could have been able to set up
  # Elasticsearch without midning security. Only when we want to go on production,
  # we had to setup security but that has changed on v8. On v8, even on
  # local development the way to setup is, security is enabled by default. So
  # if we run it without having HTTPs, without security, without a valid SSL
  # certificate, we'll get errors. So what we're going to do is, we'll disable
  # security for our local devleopment.
  # But once in the cloud, use the cloud version which is secured.
  # So, a single node cluster, at least version 8, and disable security for
  # local development!
  elasticsearch:
    container_name: elasticsearch_container
    # image: docker.elastic.co/elasticsearch/elasticsearch:8.10.4
    # image: docker.elastic.co/elasticsearch/elasticsearch:8.17.0
    # image: docker.elastic.co/elasticsearch/elasticsearch:8.12.2
    # Latest Elasticsearch Image for Docker
    image: docker.elastic.co/elasticsearch/elasticsearch:8.17.4
    restart: always
    # Elasticsearch in itself uses Java JVM. And there are some environment
    # variables that we need to set up. We don't need security enabled. So we
    # have to disable security in the configuration because security requires
    # us to have a valid SSL certificate that the Elasticsearch endpoint needs
    # to be called using HTTPS. But we don't care about all that for our local
    # development. So we've to disable security.
    environment:
      # Since Elasticsearch uses Java JVM, so we need to make sure that
      # we've Java installed on our machine!
      # We need to set `ES_JAVA_OPTS` which is just used to configure the
      # Java Virtual Machine (JVM) options for Elasticsearch when it is
      # running inside a Docker container.
      # `-Xmx1g` sets the max heap size that the JVM can use. Here, it means,
      # JVM can use upto 1GB of memory.
      # Heap is the memory space used by Elasticsearch to store data and
      # objects that the JVM needs while running.
      # `Xms1g` sets initial heap size to 1 GB i.e how much memory JVM will
      # allocate when Elasticsearch first starts. Hence, it will reserve 1 GB
      # of memory right away, rather than starting with a smaller amount and
      # growing as needed.
      ES_JAVA_OPTS: -Xmx1g -Xms1g
      # Tell Elasticsearch to lock the JVM heap in memory into RAM. This
      # prevents the operating system from swapping the memory disk, which
      # can be a performance bottleneck because when Elasticsearch is under
      # heavy load, it can handle more data on the memory rather than relying
      # on the disk, improving the performances of searches and data indexing.
      bootstrap.memory_lock: 'true'
      # Configure how Elasticsearch discovers and connects to other nodes in
      # a cluster.
      # Use single node cluster for local development / testing!
      # Elasticsearch is designed to run in a cluster, where multiple nodes
      # (Elasticsearch instances) work together to distribute data and handle
      # queries. By default, Elasticsearch tries to discover other nodes in
      # the network to form a cluster.
      # And when we set it to single-node, we're telling Elasticsearch to
      # run as a single-node cluster, meaning it will not try to discover or
      # communicate with any other nodes. It operates as a standalone instance
      discovery.type: single-node
      network.host: 0.0.0.0
      transport.host: 127.0.0.1
      http.host: 0.0.0.0
      # APM Server
      # So for the new version of APM higher than 8.11.0, security needs to
      # be enabled, network host should be set because im using the localhost
      # and then set the xpack.security.authc.token.enable to true

      # Disable security features
      # Security was disabled by default in version 7. But now, in version 8,
      # we have to manually disable it else it will request it for a valid
      # SSL certificate. We don't need to do all of that in development.
      # Hence, disabling so that it doesn't request for valid SSL certificates.
      # i.e Disable security features in Elasticsearch specifically the
      # authentication and authorization mechanism that are part of X-Pack
      # security.
      # X-Pack is a set of commercial features for Elasticsearch that enhance
      # its capabilities arouond security, monitoring, alerting and more.
      # xpack.security.enabled: 'false'
      # If i want to use authentication on metricbeat.yml, i should enable
      # this security in Elasticsearch:
      # In order to use APM server, i need to enable security.
      # Every service that tries to connect to Elasticsearch like Metricbeat,
      # Kibana, Heartbeat, APM. They all have default password `changeme`.
      # Once i enable this seurity to true, i can no more use the default password.
      # So if i enable this to true, Kibana will not connect with the `changeme`
      # password. i need to  update the changeme password and then use the
      # Kibana user.
      # And then in every case where i use the Elasticsearch password i.e
      # `changeme`, im going to udate all.
      # Everything im doing is just for my local environment so that i can run
      # APM server.
      # xpack.security.enabled: 'false'
      xpack.security.enabled: 'true'
      # With this enabled, I can create API keys using Elasticsearch's Security
      # API and use them # in my request headers for authentication. Since
      # API keys are generally considered more secure for programmatic access
      # compared to basic authentication.
      xpack.security.authc.api_key.enabled: 'true'
      # Enable the collection of monitoring data, which Elasticsearch can
      # send to a monitoring cluster. This allows us to track metrics like
      # resource usage, performance, indexing rates, etc.
      xpack.monitoring.collection.enabled: 'true'
      # Control whether the Elasticsearch node can enroll in a security-enabled
      # cluster i.e this setting is used when we're working with a security
      # enabled clusters and need to allow nodes to authenticate and securely
      # join the cluster.
      # If in a local or development environment, if security is disabled i.e
      # `xpack.security.enabled: 'fase'`, then this may not be necessary.
      # xpack.security.enrollment.enabled: 'false'
      # Need to enable for APM.
      xpack.security.enrollment.enabled: 'true'
      # Enable token based authentication for Elasticsearch which is part of
      # X-Pack Security authentication system. This is typically used in
      # production or secure environments where we want to use token-based
      # authentication for better security and scability. Hence, JWT or other
      # authentication token can be used for authenticating requests to
      # Elasticsearch, rather than requiring traditional username/password
      # authentication.
      # xpack.security.authc.token.enabled: 'true'
      # xpack.security.authc.token.enabled: 'false'
      # Need to enable for APM.
      xpack.security.authc.token.enabled: 'true'
      # Since i upadted xpack security to true, Elasticsearch Kibana will not
      # to use the default `changeme` password. So i need to change the password.
      # NOTE:
      # Whenever i stop and maybe want to restart Elasticsearch, i need to make
      # sure that i delete the volume for Elasticsearch. Because the volumes
      # contains the data, if i dont delete it and just restart it, it'll still
      # contain the previous data.
      ELASTIC_PASSWORD: stillhome

    # Elastic search runs on port 9200
    ports:
      # This is the port it uses to access the Elasticsearch REST API. But if
      # we're using multi-node cluster, the port that Elasticsearch uses to
      # communicate between those clusters is 9300:9300. But for us during
      # development, its a single-node cluster which is sufficient.
      - 9200:9200
      - 9300:9300
    deploy:
      resources:
        # Specify maximum resource a container can use
        # If the container tries to exceed these limits, Docker will throttle
        # CPU usage and may kill the container if it breaches the memory cap.
        # i.e Limits prevents a container from consuming too much CPU or memory,
        # protecting other containers and the host machine.
        limits:
          memory: 4g # Cannot use more than 4GB RAM i.e memory
          cpus: 2 # Limited to 2 CPU cores
        # Reservations are minimum guaranteed resources for the container
        # i.e Reservations guarantee that the container has enough rsources
        # to run reliably.
        reservations:
          memory: 2g # Guaranteed at least 2GB RAM/memory
          cpus: '1.5' # Guaranteed 1.5 CPU cores
    volumes:
      # Elasticsearch stores data at `/usr/share/elasticsearch/data`
      # But we want to map that to our local elasticsearch-data
      - ./docker-volumes/elasticsearch-data:/usr/share/elasticsearch/data
    # Connect Elasticsearch container to the elastic network. Doing this makes
    # sure that, Elasticsearch container can communicate with other containers
    # that are also connected to this elastic network.
    networks:
      - elastic
  kibana:
    container_name: kibana_container
    # image: docker.elastic.co/kibana/kibana:7.17.26
    # image: docker.elastic.co/kibana/kibana:8.12.2
    # Latest Kibana Image for Docker
    image: docker.elastic.co/kibana/kibana:8.17.4
    restart: always
    environment:
      # We need to set the host. And if we're using the multi node clusters,
      # then we add all the service endpoints to this list. But since we're
      # using only one node, it's going to be just one item in the list which
      # is `http://elasticsearch_container:9200`
      # In order for us to access one of these services, we use the container
      # name. And because they are in the same network, Docker knows how to
      # handle it. If they're running in different networks then that is a
      # different case. But they're running in the same network, so Docker
      # knows how to handle it. Thus, the service can be accessed with
      # unique url.
      # So with this url, Kibana will get access to the Elasticsearch service.
      - ELASTICSEARCH_HOST=['http://elasticsearch_container:9200']
      # If i just tried to restart Kibana without this login credentials, it will
      # throw an error. Error message like `Unable to retrieve information for
      # Elasticsearch nodes. security_exception`. So `missing authentication
      # credentials for REST request`. So Kibana cannot connect to Elasticsearch.
      # The reason is it tries to still use the default password. And even  if
      # i set the credentials i.e change the password, there's new error. The
      # error message is `[config validation[elasticsearch].username]: value of
      # "elastic" is forbidden. This is a superuser account`. And it says, `Use
      # a service token account instead`.
      # So what im going to do is, im going to update the password for kibana
      # user and use that and also create service account token.
      # So to update the password, im going to use `curl`. I'll do that from
      # inside the elasticsearch container. So from terminal, go into the
      # elasticsearch container and then run curl command with exec:
      # `
      # $ docker exec -it elasticsearch_container bash
      # # -H is the header and after is the api to update kibana password.
      # # i want to updatre the `/user`, and the username is `kibana_system` and
      # # i want to update the password for that `user`. and then i add the object
      # # to actually update the password.
      # # `password` is the key and `stillhome` is the password.
      # # \ is escaping the string quotes.
      # @ So this is the command that im using to update the Kibana user!
      # @ So Kibana username is is `kibana_system`.
      #
      # $ curl -s -X POST -u colson:stillhome -H "Content-Type: application/json" http://localhost:9200/_security/user/kibana_system/_password -d "{ \"password\": \"stillhome\" }"
      # `elastic` is the superuser.
      # $ curl -s -X POST -u elastic:stillhome -H "Content-Type: application/json" http://localhost:9200/_security/user/kibana_system/_password -d "{ \"password\": \"stillhome\" }"
      #
      # Now if i use this on Elasticsearch container terminal, and if i get
      # curly braces {}, that means the update was successful. But if i don't
      # get any response, then it was not applied.
      # `
      # `kibana_system` is a built in user designed for Kibana service. It has
      # the permission needed for Kibana to communicate with Elasticsearch.
      # `colson` is regular user account intended for human interaction.
      - ELASTICSEARCH_USERNAME=kibana_system
      - ELASTICSEARCH_PASSWORD=stillhome
    # Port for Kibana is 5601
    ports:
      - 5601:5601
    networks:
      - elastic
    volumes:
      # There are several methods for configuring Kibana on Docker. But
      # conventional approach is, we provide a `kibana.yml` file
      # Provide kibana.yml file to replace the `kibana.yml` located in the
      # `/usr/share/kibana/config/kibana.yml`
      # So kibana.yml file we're providing will replace the kibana inside the
      # config folder. If we dont specifiy kibana.yml file, its going to use
      # the default kibana.yml file located in the config but if we provide
      # one, it will replace/overrides.
      # The reason we're doing this is so that we can specify our Elasticsearch
      # service path i.e `[http://elasticsearch_container:9200]`
      - ./kibana.yml:/usr/share/kibana/config/kibana.yml:ro
    # We don't want Kibana to start until Elasticsearch is running because
    # Kibana depends on Elasticsearch.
    depends_on:
      - elasticsearch
  metricbeat:
    container_name: metricbeat_container
    # Note: Version numbers should all be same for Elasticsearch, Kibanna,
    # & Metricbeat.
    image: docker.elastic.co/beats/metricbeat:8.17.4
    # Setting user as root so that im able to allow metricbeat to read Docker
    # Socket.
    user: root
    # Port for Metricbeat is 5066
    ports:
      - 5066:5066
    # I use the same networks.
    networks:
      - elastic
    # Because i want to get data from Docker, i'll need to add some volumes.
    # Here the first volume will be to setup metricbeat yml file.
    volumes:
      # Im mapping this metricbeat.yml file to the metricbeat.yml inside the
      # container.
      - ./metricbeat.yml:/usr/share/metricbeat/metricbeat.yml:ro
      # @ Set up the volumes for Docker module
      # Then i need to map all the volumes.
      # So for metricbeat to have access to my Docker containers and Docker
      # resources, i map this volume.
      - /var/run/docker.sock:/var/run/docker.sock:ro
      # @ System module
      # This will get access to my system.
      - /sys/fs/cgroup:/hostfs/sys/fs/cgroup:ro
      - /proc:/hostfs/proc:ro
      - /:/hostfs:ro
    # This is some commands that i added so that metricbeat can have access to
    # my system. i.e This will allow metricbeat to get metrics from my system.
    # command: ['--strict.perms=false', '-system.hostfs=/hostfs']
    command: ['--strict.perms=false', '--system.hostfs=/hostfs']
    depends_on:
      - elasticsearch
  heartbeat:
    container_name: heartbeat_container
    # image: docker.elastic.co/beats/metricbeat:8.11.0
    # @ Latest Heartbeat image
    image: docker.elastic.co/beats/heartbeat:8.17.4
    user: root
    # I dont need the port.
    hostname: heartbeat
    # extra_hosts:
    #   # This is important for LINUX machine.
    #   - 'host.docker.internal:host-gateway'
    cap_add:
      # This is some capabilities added to docker for this particular container.
      - NET_RAW
    # I use the same network.
    networks:
      - elastic
    command: ['--strict.perms=false']
    volumes:
      # Replace what i have in the container i.e heartbeat.yml with my
      # file heartbeat.yml.
      - ./heartbeat.yml:/usr/share/heartbeat/heartbeat.yml:ro
    depends_on:
      - elasticsearch

  gateway:
    container_name: gateway_container
    # THis is where we need to set the Docker image that it requires or the
    # Docker file it needs to use
    build:
      # context is where we have the file
      context: ../server/1-api-gateway-service
      dockerfile: Dockerfile.dev
    restart: always
    ports:
      - 4000:4000
    env_file: ../server/1-api-gateway-service/.env
    environment:
      - ENABLE_APM=0
      - GATEWAY_JWT_TOKEN=2cbbd6e5ac45ba08f6f4ac745bc6810fbc2c21070e8388c76846096862a47b8e
      - JWT_TOKEN=60c60e1f014355049c2493d8d24aacd5e81e7fde4c4e0e3e349f3fe7c8b7fed3
      - NODE_ENV=development
      - SECRET_KEY_ONE=580af2ec4469679258771e8a7ff58cbec152b69dfbb66040a0904688ebc520ec
      - SECRET_KEY_TWO=933861b15c6dfe53cef4abeb77b42fdb933900d43c74ac95f9dec7cd743f3a01
      - CLIENT_URL=http://localhost:3000
      - AUTH_BASE_URL=http://auth_container:4002
      - USERS_BASE_URL=http://localhost:4003
      - GIG_BASE_URL=http://localhost:4004
      - MESSAGE_BASE_URL=http://localhost:4005
      - ORDER_BASE_URL=http://localhost:4006
      - REVIEW_BASE_URL=http://localhost:4007
      - REDIS_HOST=redis://redis_container:6379
      - ELASTIC_SEARCH_URL=http://elasticsearch_container:9200
      - ELASTIC_APM_SERVER_URL=http://localhost:8200
      - ELASTIC_APM_SECRET_TOKEN=
    depends_on:
      - elasticsearch
  notifications:
    container_name: notification_container
    # THis is where we need to set the Docker image that it requires or the
    # Docker file it needs to use
    build:
      # context is where we have the file
      context: ../server/2-notification-service
      dockerfile: Dockerfile.dev
    restart: always
    ports:
      - 4001:4001
    env_file: ../server/2-notification-service/.env
    environment:
      - ENABLE_APM=0
      - NODE_ENV=development
      - CLIENT_URL=http://localhost:3000
      - RABBITMQ_ENDPOINT=amqp://colson:stillhome@rabbitmq_container:5672
      - SENDER_EMAIL=warren.smith@ethereal.email
      - SENDER_EMAIL_PASSWORD=2JgKZu9VmbKMxQuwGn
      - ELASTIC_SEARCH_URL=http://elasticsearch_container:9200
      - ELASTIC_APM_SERVER_URL=http://localhost:8200
      - ELASTIC_APM_SECRET_TOKEN=
    depends_on:
      - elasticsearch
  auth:
    container_name: auth_container
    # THis is where we need to set the Docker image that it requires or the
    # Docker file it needs to use
    build:
      # context is where we have the file
      context: ../server/3-auth-service
      dockerfile: Dockerfile.dev
    restart: always
    ports:
      - 4002:4002
    env_file: ../server/3-auth-service/.env
    environment:
      - ENABLE_APM=0
      - GATEWAY_JWT_TOKEN=2cbbd6e5ac45ba08f6f4ac745bc6810fbc2c21070e8388c76846096862a47b8e
      - JWT_TOKEN=60c60e1f014355049c2493d8d24aacd5e81e7fde4c4e0e3e349f3fe7c8b7fed3
      - NODE_ENV=development
      - API_GATEWAY_URL=http://gateway_container:4000
      - CLIENT_URL=http://localhost:3000
      - RABBITMQ_ENDPOINT=amqp://colson:stillhome@localhost:5672
      - MYSQL_DB=mysql://colson:stillhome@mysql_container:3306/tradenexus_auth
      - CLOUD_NAME=colson0x1
      - CLOUD_API_KEY=211615812663681
      - CLOUD_API_SECRET=RrHVkR9yD8bQpwKMUQE9FXmSlSk
      - ELASTIC_SEARCH_URL=http://elasticsearch_container:9200
      - ELASTIC_APM_SERVER_URL=http://localhost:8200
      - ELASTIC_APM_SECRET_TOKEN=
    depends_on:
      - elasticsearch
      - mysql
  users:
    container_name: users_container
    # THis is where we need to set the Docker image that it requires or the
    # Docker file it needs to use
    build:
      # context is where we have the file
      context: ../server/4-users-service
      dockerfile: Dockerfile.dev
    restart: always
    ports:
      - 4003:4003
    env_file: ../server/4-users-service/.env
    environment:
      - ENABLE_APM=0
      - DATABASE_URL=mongodb://mongodb_container:27017/tradenexus-users
      - GATEWAY_JWT_TOKEN=2cbbd6e5ac45ba08f6f4ac745bc6810fbc2c21070e8388c76846096862a47b8e
      - JWT_TOKEN=60c60e1f014355049c2493d8d24aacd5e81e7fde4c4e0e3e349f3fe7c8b7fed3
      - NODE_ENV=development
      - API_GATEWAY_URL=http://gateway_container:4000
      - RABBITMQ_ENDPOINT=amqp://colson:stillhome@rabbitmq_container:5672
      - CLOUD_NAME=colson0x1
      - CLOUD_API_KEY=211615812663681
      - CLOUD_API_SECRET=RrHVkR9yD8bQpwKMUQE9FXmSlSk
      - REDIS_HOST=redis://redis_container:6379
      - ELASTIC_SEARCH_URL=http://elasticsearch_container:9200
      - ELASTIC_APM_SERVER_URL=http://localhost:8200
      - ELASTIC_APM_SECRET_TOKEN=
    depends_on:
      - elasticsearch
      - mongodb
  gig:
    container_name: gig_container
    build:
      context: ../server/5-gig-service
      dockerfile: Dockerfile.dev
    restart: always
    ports:
      - 4004:4004
    env_file: ../server/5-gig-service/.env
    environment:
      - ENABLE_APM=0
      - DATABASE_URL=mongodb://127.0.0.1:27017/tradenexus-gig
      - GATEWAY_JWT_TOKEN=2cbbd6e5ac45ba08f6f4ac745bc6810fbc2c21070e8388c76846096862a47b8e
      - JWT_TOKEN=60c60e1f014355049c2493d8d24aacd5e81e7fde4c4e0e3e349f3fe7c8b7fed3
      - NODE_ENV=development
      - API_GATEWAY_URL=http://localhost:4000
      - RABBITMQ_ENDPOINT=amqp://colson:stillhome@localhost:5672
      - CLOUD_NAME=colson0x1
      - CLOUD_API_KEY=211615812663681
      - CLOUD_API_SECRET=RrHVkR9yD8bQpwKMUQE9FXmSlSk
      - REDIS_HOST=redis://localhost:6379
      - ELASTIC_SEARCH_URL=http://localhost:9200
      - ELASTIC_APM_SERVER_URL=http://localhost:8200
      - ELASTIC_APM_SECRET_TOKEN=
    depends_on:
      - elasticsearch
      - mongodb
  chat:
    container_name: chat_container
    build:
      context: ../server/6-chat-service
      dockerfile: Dockerfile.dev
    restart: always
    ports:
      - 4005:4005
    env_file: ../server/6-chat-service/.env
    environment:
      - ENABLE_APM=0
      - DATABASE_URL=mongodb://127.0.0.1:27017/tradenexus-chat
      - GATEWAY_JWT_TOKEN=2cbbd6e5ac45ba08f6f4ac745bc6810fbc2c21070e8388c76846096862a47b8e
      - JWT_TOKEN=60c60e1f014355049c2493d8d24aacd5e81e7fde4c4e0e3e349f3fe7c8b7fed3
      - NODE_ENV=development
      - API_GATEWAY_URL=http://localhost:4000
      - RABBITMQ_ENDPOINT=amqp://colson:stillhome@localhost:5672
      - CLOUD_NAME=colson0x1
      - CLOUD_API_KEY=211615812663681
      - CLOUD_API_SECRET=RrHVkR9yD8bQpwKMUQE9FXmSlSk
      - ELASTIC_SEARCH_URL=http://localhost:9200
      - ELASTIC_APM_SERVER_URL=http://localhost:8200
      - ELASTIC_APM_SECRET_TOKEN=
    depends_on:
      - elasticsearch
      - mongodb
  order:
    container_name: order_container
    build:
      context: ../server/7-order-service
      dockerfile: Dockerfile.dev
    restart: always
    ports:
      - 4006:4006
    env_file: ../server/7-order-service/.env
    environment:
      - ENABLE_APM=0
      - DATABASE_URL=mongodb://127.0.0.1:27017/tradenexus-order
      - GATEWAY_JWT_TOKEN=2cbbd6e5ac45ba08f6f4ac745bc6810fbc2c21070e8388c76846096862a47b8e
      - JWT_TOKEN=60c60e1f014355049c2493d8d24aacd5e81e7fde4c4e0e3e349f3fe7c8b7fed3
      - NODE_ENV=development
      - API_GATEWAY_URL=http://localhost:4000
      - CLIENT_URL=http://localhost:3000
      - RABBITMQ_ENDPOINT=amqp://colson:stillhome@localhost:5672
      - STRIPE_API_KEY=
      - CLOUD_NAME=colson0x1
      - CLOUD_API_KEY=211615812663681
      - CLOUD_API_SECRET=RrHVkR9yD8bQpwKMUQE9FXmSlSk
      - ELASTIC_SEARCH_URL=http://localhost:9200
      - ELASTIC_APM_SERVER_URL=http://localhost:8200
      - ELASTIC_APM_SECRET_TOKEN=
    depends_on:
      - elasticsearch
      - mongodb
  review:
    container_name: review_container
    build:
      context: ../server/8-review-service
      dockerfile: Dockerfile.dev
    restart: always
    ports:
      - 4007:4007
    env_file: ../server/8-review-service/.env
    environment:
      - ENABLE_APM=0
      - DATABASE_HOST=127.0.0.1
      - DATABASE_USER=colson
      - DATABASE_PASSWORD=stillhome
      - DATABASE_NAME=tradenexus_reviews
      - GATEWAY_JWT_TOKEN=2cbbd6e5ac45ba08f6f4ac745bc6810fbc2c21070e8388c76846096862a47b8e
      - JWT_TOKEN=60c60e1f014355049c2493d8d24aacd5e81e7fde4c4e0e3e349f3fe7c8b7fed3
      - NODE_ENV=development
      - API_GATEWAY_URL=http://localhost:4000
      - RABBITMQ_ENDPOINT=amqp://colson:stillhome@localhost:5672
      - CLOUD_NAME=colson0x1
      - CLOUD_API_KEY=211615812663681
      - CLOUD_API_SECRET=RrHVkR9yD8bQpwKMUQE9FXmSlSk
      - ELASTIC_SEARCH_URL=http://localhost:9200
      - ELASTIC_APM_SERVER_URL=http://localhost:8200
      - ELASTIC_APM_SECRET_TOKEN=
    depends_on:
      - elasticsearch
      - postgres

# Tell Docker Compose to create a network called elastic so that any
# service (container) that needs to communicate with other services
# in the same network will be connected to this elastic network.
networks:
  elastic:
    name: elastic
